{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt        <|im_start|>system\\nYou are a helpful assistan...\n",
      "completion     \\nFrom the information provided, here's what ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ds = pd.read_parquet(\"/home/users/hc387/data/sft_data/Qwen3b-instruct/correct/zebra_puzzles.parquet\")\n",
    "print(ds.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/hc387/miniconda3/envs/zero/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/users/hc387/miniconda3/envs/zero/lib/python3.9/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineStats:\n",
    "    \"\"\"\n",
    "    Maintains statistics using an online update strategy:\n",
    "      - running_mean: Average value for each feature dimension\n",
    "      - running_var:  Variance approximation for each feature dimension\n",
    "      - running_l2:   Mean of squared values (L2) for each feature dimension\n",
    "\n",
    "    The idea is that when receiving a new batch x, we scale the existing statistics and blend them with the new batch \n",
    "    in the reverse proportion. This allows us to dynamically estimate the mean, approximate variance, and L2 mean \n",
    "    without storing all the sample data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim=None):\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.running_mean = None  # [D], running mean of features\n",
    "        self.running_var  = None  # [D], running variance of features\n",
    "        self.running_l2   = None  # [D], mean of squared features (L2)\n",
    "        self.sample_count = 0     # Total number of samples (including batch and sequence dimensions)\n",
    "\n",
    "    def update(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Update the statistics using a new batch x.\n",
    "        Supports input shapes of (batch, length, dim) or (N, dim); internally flattened to (N, D).\n",
    "\n",
    "        Steps:\n",
    "          1. Flatten to [N, D]\n",
    "          2. Clamp to avoid extreme values or overflows\n",
    "          3. If it's the first update, directly initialize the statistics\n",
    "          4. If statistics already exist, update them using online merging strategy\n",
    "        \"\"\"\n",
    "        # 1. Flatten => [N, D]\n",
    "        x = x.view(-1, x.shape[-1])\n",
    "\n",
    "        # 2. Clamp to avoid extreme values causing floating point overflow\n",
    "        x = x.to(torch.float32)  # Or x.float()\n",
    "\n",
    "        # Debug: Check for Inf/NaN\n",
    "        if torch.isinf(x).any():\n",
    "            print(f\"[OnlineStats] After clamp, x still has Inf, shape={x.shape}\")\n",
    "        if torch.isnan(x).any():\n",
    "            print(f\"[OnlineStats] x has NaN after clamp, shape={x.shape}\")\n",
    "\n",
    "        n_new = x.shape[0]\n",
    "        \n",
    "        # 3. If it's the first update, initialize the statistics\n",
    "        if self.sample_count == 0:\n",
    "            self.hidden_dim = x.shape[-1]\n",
    "            self.running_mean = x.mean(dim=0)        # [D]\n",
    "            self.running_var  = torch.zeros_like(self.running_mean)\n",
    "            self.running_l2   = (x ** 2).mean(dim=0) # [D]\n",
    "            self.sample_count = n_new\n",
    "            return\n",
    "\n",
    "        # 4. If statistics already exist, update using online merging\n",
    "        old_mean = self.running_mean.clone()\n",
    "        total_count = self.sample_count + n_new\n",
    "\n",
    "        alpha_old = self.sample_count / total_count\n",
    "        alpha_new = n_new / total_count\n",
    "\n",
    "        batch_mean = x.mean(dim=0)  # [D]\n",
    "        self.running_mean = alpha_old * self.running_mean + alpha_new * batch_mean\n",
    "\n",
    "        # 4.1 Update variance (running_var)\n",
    "        if total_count > 1:\n",
    "            self.running_var *= (self.sample_count - 1) / (total_count - 1)\n",
    "\n",
    "        diff_sum = ((x - self.running_mean) * (x - old_mean)).sum(dim=0)  # [D]\n",
    "\n",
    "        self.running_var += diff_sum / total_count\n",
    "\n",
    "        # 4.2 Update L2 mean\n",
    "        batch_l2 = (x ** 2).mean(dim=0)  # [D]\n",
    "        self.running_l2 = alpha_old * self.running_l2 + alpha_new * batch_l2\n",
    "\n",
    "        # 4.3 Update sample count\n",
    "        self.sample_count = total_count\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"\n",
    "        Return the current statistics: mean, variance, and L2.\n",
    "        \"\"\"\n",
    "        if self.sample_count == 0:\n",
    "            return {\n",
    "                \"mean\": None,\n",
    "                \"var\":  None,\n",
    "                \"l2\":   None\n",
    "            }\n",
    "        return {\n",
    "            \"mean\": self.running_mean,\n",
    "            \"var\":  self.running_var,\n",
    "            \"l2\":   self.running_l2\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the statistics and clear recorded data.\n",
    "        \"\"\"\n",
    "        self.running_mean = None\n",
    "        self.running_var  = None\n",
    "        self.running_l2   = None\n",
    "        self.sample_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_activations = {}\n",
    "\n",
    "def init_stats_dict():\n",
    "    return {\n",
    "        'attention_input_states': OnlineStats(),\n",
    "        'attention_post_aggregation': OnlineStats(),\n",
    "        'mlp_input_states': OnlineStats(),\n",
    "        'mlp_intermediate_states': OnlineStats()\n",
    "    }\n",
    "\n",
    "def q_proj_hook(module, input, output):\n",
    "    layer_activations[layer_idx]['attention_input_states'].update(\n",
    "        input[0].detach().cpu()\n",
    "    )\n",
    "\n",
    "def o_proj_hook(module, input, output):\n",
    "    layer_activations[layer_idx]['attention_post_aggregation'].update(\n",
    "        input[0].detach().cpu()\n",
    "    )\n",
    "\n",
    "def gate_proj_hook(module, input, output):\n",
    "    layer_activations[layer_idx]['mlp_input_states'].update(\n",
    "        input[0].detach().cpu()\n",
    "    )\n",
    "\n",
    "def down_proj_hook(module, input, output):\n",
    "    layer_activations[layer_idx]['mlp_intermediate_states'].update(\n",
    "        input[0].detach().cpu()\n",
    "    )\n",
    "\n",
    "\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    layer_activations[i] = init_stats_dict()\n",
    "    layer.self_attn.q_proj.register_forward_hook(q_proj_hook)\n",
    "    layer.self_attn.o_proj.register_forward_hook(o_proj_hook)\n",
    "    layer.mlp.gate_proj.register_forward_hook(gate_proj_hook)\n",
    "    layer.mlp.down_proj.register_forward_hook(down_proj_hook)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
